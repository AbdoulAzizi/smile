<!DOCTYPE html >
<html>
        <head>
          <title>vq - Smile - Statistical Machine Intelligence and Learning Engine - smile.vq</title>
          <meta name="description" content="vq - Smile - Statistical Machine Intelligence and Learning Engine - smile.vq" />
          <meta name="keywords" content="vq Smile Statistical Machine Intelligence and Learning Engine smile.vq" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      <link href="../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../lib/jquery.js" id="jquery-js"></script>
      <script type="text/javascript" src="../../lib/jquery-ui.js"></script>
      <script type="text/javascript" src="../../lib/template.js"></script>
      <script type="text/javascript" src="../../lib/tools.tooltip.js"></script>
      
      <script type="text/javascript">
         if(top === self) {
            var url = '../../index.html';
            var hash = 'smile.vq.package';
            var anchor = window.location.hash;
            var anchor_opt = '';
            if (anchor.length >= 1)
              anchor_opt = '@' + anchor.substring(1);
            window.location.href = url + '#' + hash + anchor_opt;
         }
   	  </script>
    
        </head>
        <body class="value">
      <div id="definition">
        <img alt="Package" src="../../lib/package_big.png" />
        <p id="owner"><a href="../package.html" class="extype" name="smile">smile</a></p>
        <h1>vq</h1><span class="permalink">
      <a href="../../index.html#smile.vq.package" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">vq</span>
      </span>
      </h4>
      
          <div id="comment" class="fullcommenttop"><div class="comment cmt"><p>Originally used for data compression, Vector quantization (VQ)
allows the modeling of probability density functions by
the distribution of prototype vectors. It works by dividing a large set of points
(vectors) into groups having approximately the same number of
points closest to them. Each group is represented by its centroid
point, as in K-Means and some other clustering algorithms.</p><p>Vector quantization is is based on the competitive learning paradigm,
and also closely related to sparse coding models
used in deep learning algorithms such as autoencoder.</p><p>Algorithms in this package also support the <code>partition</code>
method for clustering purpose.
</p></div><div class="toggleContainer block">
          <span class="toggle">Linear Supertypes</span>
          <div class="superTypes hiddenContent"><a href="Operators.html" class="extype" name="smile.vq.Operators">Operators</a>, <span class="extype" name="scala.AnyRef">AnyRef</span>, <span class="extype" name="scala.Any">Any</span></div>
        </div></div>
        

      <div id="mbrsel">
        <div id="textfilter"><span class="pre"></span><span class="input"><input id="mbrsel-input" type="text" accesskey="/" /></span><span class="post"></span></div>
        <div id="order">
              <span class="filtertype">Ordering</span>
              <ol>
                
                <li class="alpha in"><span>Alphabetic</span></li>
                <li class="inherit out"><span>By Inheritance</span></li>
              </ol>
            </div>
        <div id="ancestors">
                <span class="filtertype">Inherited<br />
                </span>
                <ol id="linearization">
                  <li class="in" name="smile.vq"><span>vq</span></li><li class="in" name="smile.vq.Operators"><span>Operators</span></li><li class="in" name="scala.AnyRef"><span>AnyRef</span></li><li class="in" name="scala.Any"><span>Any</span></li>
                </ol>
              </div><div id="ancestors">
            <span class="filtertype"></span>
            <ol>
              <li class="hideall out"><span>Hide All</span></li>
              <li class="showall in"><span>Show All</span></li>
            </ol>
          </div>
        <div id="visbl">
            <span class="filtertype">Visibility</span>
            <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
          </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        <div id="types" class="types members">
              <h3>Type Members</h3>
              <ol><li name="smile.vq.Operators" visbl="pub" data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="OperatorsextendsAnyRef"></a>
      <a id="Operators:Operators"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a href="Operators.html"><span class="name">Operators</span></a><span class="result"> extends <span class="extype" name="scala.AnyRef">AnyRef</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#smile.vq.package@OperatorsextendsAnyRef" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">High level vector quantization operators.</p>
    </li></ol>
            </div>

        

        <div id="values" class="values members">
              <h3>Value Members</h3>
              <ol><li name="smile.vq.Operators#gng" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="gng(data:Array[Array[Double]],epochs:Int,epsBest:Double,epsNeighbor:Double,maxEdgeAge:Int,lambda:Int,alpha:Double,beta:Double):smile.vq.GrowingNeuralGas"></a>
      <a id="gng(Array[Array[Double]],Int,Double,Double,Int,Int,Double,Double):GrowingNeuralGas"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">gng</span><span class="params">(<span name="data">data: <span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Double">Double</span>]]</span>, <span name="epochs">epochs: <span class="extype" name="scala.Int">Int</span> = <span class="symbol">25</span></span>, <span name="epsBest">epsBest: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.05</span></span>, <span name="epsNeighbor">epsNeighbor: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.0006</span></span>, <span name="maxEdgeAge">maxEdgeAge: <span class="extype" name="scala.Int">Int</span> = <span class="symbol">88</span></span>, <span name="lambda">lambda: <span class="extype" name="scala.Int">Int</span> = <span class="symbol">300</span></span>, <span name="alpha">alpha: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.5</span></span>, <span name="beta">beta: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.9995</span></span>)</span><span class="result">: <span class="extype" name="smile.vq.GrowingNeuralGas">GrowingNeuralGas</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#smile.vq.package@gng(data:Array[Array[Double]],epochs:Int,epsBest:Double,epsNeighbor:Double,maxEdgeAge:Int,lambda:Int,alpha:Double,beta:Double):smile.vq.GrowingNeuralGas" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Growing Neural Gas.</p><div class="fullcomment"><div class="comment cmt"><p>Growing Neural Gas. As an extension of Neural Gas, Growing Neural Gas
can add and delete nodes during algorithm execution.  The growth mechanism
is based on growing cell structures and competitive Hebbian learning.</p><p>Compared to Neural Gas, GNG has the following distinctions:</p><ul><li>The system has the ability to add and delete nodes.</li><li>Local Error measurements are noted at each step helping it to locally
insert/delete nodes.</li><li>Edges are connected between nodes, so a sufficiently old edges is
deleted. Such edges are intended place holders for localized data distribution.</li><li>Such edges also help to locate distinct clusters (those clusters are
not connected by edges).</li></ul><h6>References:</h6><ul><li>B. Fritzke. A growing neural gas network learns topologies. NIPS, 1995.
</li></ul></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>the data set.</p></dd><dt class="param">epochs</dt><dd class="cmt"><p>the number of epochs of learning.</p></dd><dt class="param">epsBest</dt><dd class="cmt"><p>the fraction to update nearest neuron.</p></dd><dt class="param">epsNeighbor</dt><dd class="cmt"><p>the fraction to update neighbors of nearest neuron.</p></dd><dt class="param">maxEdgeAge</dt><dd class="cmt"><p>the maximum age of edges.</p></dd><dt class="param">lambda</dt><dd class="cmt"><p>if the number of input signals so far is an integer multiple
              of lambda, insert a new neuron.</p></dd><dt class="param">alpha</dt><dd class="cmt"><p>decrease error variables by multiplying them with alpha
             during inserting a new neuron.</p></dd><dt class="param">beta</dt><dd class="cmt"><p>decrease all error variables by multiply them with beta.</p></dd></dl><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="Operators.html" class="extype" name="smile.vq.Operators">Operators</a></dd></dl></div>
    </li><li name="smile.vq.Operators#neuralgas" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="neuralgas(data:Array[Array[Double]],k:Int,lambda_i:Double,lambda_f:Double,eps_i:Double,eps_f:Double,steps:Int):smile.vq.NeuralGas"></a>
      <a id="neuralgas(Array[Array[Double]],Int,Double,Double,Double,Double,Int):NeuralGas"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">neuralgas</span><span class="params">(<span name="data">data: <span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Double">Double</span>]]</span>, <span name="k">k: <span class="extype" name="scala.Int">Int</span></span>, <span name="lambda_i">lambda_i: <span class="extype" name="scala.Double">Double</span></span>, <span name="lambda_f">lambda_f: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.01</span></span>, <span name="eps_i">eps_i: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.5</span></span>, <span name="eps_f">eps_f: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.005</span></span>, <span name="steps">steps: <span class="extype" name="scala.Int">Int</span> = <span class="symbol">25</span></span>)</span><span class="result">: <span class="extype" name="smile.vq.NeuralGas">NeuralGas</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#smile.vq.package@neuralgas(data:Array[Array[Double]],k:Int,lambda_i:Double,lambda_f:Double,eps_i:Double,eps_f:Double,steps:Int):smile.vq.NeuralGas" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Neural Gas soft competitive learning algorithm.</p><div class="fullcomment"><div class="comment cmt"><p>Neural Gas soft competitive learning algorithm. The Neural Gas is inspired
by the Self-Organizing Map for finding optimal data representations based on
feature vectors. The algorithm was coined &quot;Neural Gas&quot; because of the
dynamics of the feature vectors during the adaptation process, which
distribute themselves like a gas within the data space. Although it is mainly
applied where data compression or vector quantization is an issue,
it is also used for cluster analysis as a robustly converging alternative to
the k-means clustering. A prominent extension is the Growing Neural Gas.</p><p>Compared to SOM, neural gas has no topology of a fixed dimensionality
(in fact, no topology at all). For each input signal during learning, the
neural gas algorithm sorts the neurons of the network according to the
distance of their reference vectors to the input signal. Based on this
&quot;rank order&quot;, neurons are adapted based on the adaptation strength that are
decreased according to a fixed schedule.</p><p>The adaptation step of the Neural Gas can be interpreted as gradient descent
on a cost function. By adapting not only the closest feature vector but all
of them with a step size decreasing with increasing distance order,
compared to k-means clustering, a much more robust convergence of the
algorithm can be achieved.</p><h6>References:</h6><ul><li>Thomas Martinetz and Klaus Schulten. A &quot;neural gas&quot; network learns topologies. Artificial Neural Networks, 397-402, 1991.</li><li>T. Martinetz, S. Berkovich, and K. Schulten. &quot;Neural-gas&quot; Network for Vector Quantization and its Application to Time-Series Prediction. IEEE Trans. on Neural Networks, 4(4):558-569, 1993.</li><li>T. Martinetz and K. Schulten. Topology representing networks. Neural Networks, 7(3):507-522, 1994.
</li></ul></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>the data set.</p></dd><dt class="param">k</dt><dd class="cmt"><p>the number of units in the neural gas.</p></dd><dt class="param">lambda_i</dt><dd class="cmt"><p>the initial value of lambda. lambda_i and lambda_f are
                used to set the soft learning radius/rate, i.e. determining the number
                of neural units significantly changing their synaptic weights with
                each adaptation step.</p></dd><dt class="param">lambda_f</dt><dd class="cmt"><p>The final value of lambda.</p></dd><dt class="param">eps_i</dt><dd class="cmt"><p>the initial value of epsilon. epsilon_i and epsilon_f
             are the initial and final learning rate respectively.</p></dd><dt class="param">eps_f</dt><dd class="cmt"><p>the final value of epsilon.</p></dd><dt class="param">steps</dt><dd class="cmt"><p>the number of iterations. Note that for one iteration, we
             mean that the learning process goes through the whole dataset.</p></dd></dl><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="Operators.html" class="extype" name="smile.vq.Operators">Operators</a></dd></dl></div>
    </li><li name="smile.vq.Operators#neuralmap" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="neuralmap(data:Array[Array[Double]],radius:Double,L:Int,k:Int,epsBest:Double,epsNeighbor:Double):smile.vq.NeuralMap"></a>
      <a id="neuralmap(Array[Array[Double]],Double,Int,Int,Double,Double):NeuralMap"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">neuralmap</span><span class="params">(<span name="data">data: <span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Double">Double</span>]]</span>, <span name="radius">radius: <span class="extype" name="scala.Double">Double</span></span>, <span name="L">L: <span class="extype" name="scala.Int">Int</span></span>, <span name="k">k: <span class="extype" name="scala.Int">Int</span></span>, <span name="epsBest">epsBest: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.05</span></span>, <span name="epsNeighbor">epsNeighbor: <span class="extype" name="scala.Double">Double</span> = <span class="symbol">0.0006</span></span>)</span><span class="result">: <span class="extype" name="smile.vq.NeuralMap">NeuralMap</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#smile.vq.package@neuralmap(data:Array[Array[Double]],radius:Double,L:Int,k:Int,epsBest:Double,epsNeighbor:Double):smile.vq.NeuralMap" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">NeuralMap is an efficient competitive learning algorithm inspired by growing
neural gas and BIRCH.</p><div class="fullcomment"><div class="comment cmt"><p>NeuralMap is an efficient competitive learning algorithm inspired by growing
neural gas and BIRCH. Like growing neural gas, NeuralMap has the ability to
add and delete neurons with competitive Hebbian learning. Edges exist between
neurons close to each other. The edges are intended place holders for
localized data distribution. The edges also help to locate distinct clusters
(those clusters are not connected by edges). NeuralMap employs Locality-Sensitive
Hashing to speedup the learning while BIRCH uses balanced CF trees.
</p></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>the data set.</p></dd><dt class="param">radius</dt><dd class="cmt"><p>the distance radius to activate a neuron for a given signal.</p></dd><dt class="param">L</dt><dd class="cmt"><p>the number of hash tables.</p></dd><dt class="param">k</dt><dd class="cmt"><p>the number of random projection hash functions.</p></dd><dt class="param">epsBest</dt><dd class="cmt"><p>the fraction to update activated neuron.</p></dd><dt class="param">epsNeighbor</dt><dd class="cmt"><p>the fraction to update neighbors of activated neuron.</p></dd></dl><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="Operators.html" class="extype" name="smile.vq.Operators">Operators</a></dd></dl></div>
    </li><li name="smile.vq.Operators#som" visbl="pub" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="som(data:Array[Array[Double]],width:Int,height:Int):smile.vq.SOM"></a>
      <a id="som(Array[Array[Double]],Int,Int):SOM"></a>
      <h4 class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">def</span>
      </span>
      <span class="symbol">
        <span class="name">som</span><span class="params">(<span name="data">data: <span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Array">Array</span>[<span class="extype" name="scala.Double">Double</span>]]</span>, <span name="width">width: <span class="extype" name="scala.Int">Int</span></span>, <span name="height">height: <span class="extype" name="scala.Int">Int</span></span>)</span><span class="result">: <span class="extype" name="smile.vq.SOM">SOM</span></span>
      </span>
      </h4><span class="permalink">
      <a href="../../index.html#smile.vq.package@som(data:Array[Array[Double]],width:Int,height:Int):smile.vq.SOM" title="Permalink" target="_top">
        <img src="../../lib/permalink.png" alt="Permalink" />
      </a>
    </span>
      <p class="shortcomment cmt">Self-Organizing Map.</p><div class="fullcomment"><div class="comment cmt"><p>Self-Organizing Map. An SOM is a unsupervised learning method to produce
a low-dimensional (typically two-dimensional) discretized representation
(called a map) of the input space of the training samples. The model was
first described as an artificial neural network by Teuvo Kohonen, and is
sometimes called a Kohonen map.</p><p>While it is typical to consider SOMs as related to feed-forward networks where
the nodes are visualized as being attached, this type of architecture is
fundamentally different in arrangement and motivation because SOMs use a
neighborhood function to preserve the topological properties of the input
space. This makes SOMs useful for visualizing low-dimensional views of
high-dimensional data, akin to multidimensional scaling.</p><p>SOMs belong to a large family of competitive learning process and vector
quantization. An SOM consists of components called nodes or neurons.
Associated with each node is a weight vector of the same dimension as
the input data vectors and a position in the map space. The usual arrangement
of nodes is a regular spacing in a hexagonal or rectangular grid. The
self-organizing map describes a mapping from a higher dimensional input
space to a lower dimensional map space. During the (iterative) learning,
the input vectors are compared to the weight vector of each neuron. Neurons
who most closely match the input are known as the best match unit (BMU) of
the system. The weight vector of the BMU and those of nearby neurons are
adjusted to be closer to the input vector by a certain step size.</p><p>There are two ways to interpret a SOM. Because in the training phase weights
of the whole neighborhood are moved in the same direction, similar items
tend to excite adjacent neurons. Therefore, SOM forms a semantic map where
similar samples are mapped close together and dissimilar apart.
The other way is to think of neuronal weights as pointers to the input space.
They form a discrete approximation of the distribution of training samples.
More neurons point to regions with high training sample concentration and
fewer where the samples are scarce.</p><p>SOM may be considered a nonlinear generalization of Principal components
analysis (PCA). It has been shown, using both artificial and real
geophysical data, that SOM has many advantages over the conventional feature
extraction methods such as Empirical Orthogonal Functions (EOF) or PCA.</p><p>It has been shown that while SOMs with a small number of nodes behave in a
way that is similar to K-means. However, larger SOMs rearrange data
in a way that is fundamentally topological in character and display properties
which are emergent. Therefore, large maps are preferable to smaller ones.
In maps consisting of thousands of nodes, it is possible to perform cluster
operations on the map itself.</p><p>A common way to display SOMs is the heat map of U-matrix. The U-matrix value
of a particular node is the minimum/maximum/average distance between the node
and its closest neighbors. In a rectangular grid for instance, we might
consider the closest 4 or 8 nodes.</p><h6>References:</h6><ul><li>Teuvo KohonenDan. Self-organizing maps. Springer, 3rd edition, 2000.
</li></ul></div><dl class="paramcmts block"><dt class="param">data</dt><dd class="cmt"><p>the dataset for clustering.</p></dd><dt class="param">width</dt><dd class="cmt"><p>the width of map.</p></dd><dt class="param">height</dt><dd class="cmt"><p>the height of map.</p></dd></dl><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="Operators.html" class="extype" name="smile.vq.Operators">Operators</a></dd></dl></div>
    </li></ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        <div class="parent" name="smile.vq.Operators">
              <h3>Inherited from <a href="Operators.html" class="extype" name="smile.vq.Operators">Operators</a></h3>
            </div><div class="parent" name="scala.AnyRef">
              <h3>Inherited from <span class="extype" name="scala.AnyRef">AnyRef</span></h3>
            </div><div class="parent" name="scala.Any">
              <h3>Inherited from <span class="extype" name="scala.Any">Any</span></h3>
            </div>
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>


    </body>
      </html>
